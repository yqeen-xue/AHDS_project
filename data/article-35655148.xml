<?xml version="1.0" ?>
<!DOCTYPE PubmedArticleSet PUBLIC "-//NLM//DTD PubMedArticle, 1st January 2024//EN" "https://dtd.nlm.nih.gov/ncbi/pubmed/out/pubmed_240101.dtd">
<PubmedArticleSet>
<PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">35655148</PMID><DateCompleted><Year>2022</Year><Month>06</Month><Day>06</Day></DateCompleted><DateRevised><Year>2024</Year><Month>08</Month><Day>29</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">1471-2105</ISSN><JournalIssue CitedMedium="Internet"><Volume>23</Volume><Issue>1</Issue><PubDate><Year>2022</Year><Month>Jun</Month><Day>02</Day></PubDate></JournalIssue><Title>BMC bioinformatics</Title><ISOAbbreviation>BMC Bioinformatics</ISOAbbreviation></Journal><ArticleTitle>CoQUAD: a COVID-19 question answering dataset system, facilitating research, benchmarking, and practice.</ArticleTitle><Pagination><StartPage>210</StartPage><MedlinePgn>210</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">210</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1186/s12859-022-04751-6</ELocationID><Abstract><AbstractText Label="BACKGROUND" NlmCategory="BACKGROUND">Due to the growing amount of COVID-19 research literature, medical experts, clinical scientists, and researchers frequently struggle to stay up to date on the most recent findings. There is a pressing need to assist researchers and practitioners in mining and responding to COVID-19-related questions on time.</AbstractText><AbstractText Label="METHODS" NlmCategory="METHODS">This paper introduces CoQUAD, a question-answering system that can extract answers related to COVID-19 questions in an efficient manner. There are two datasets provided in this work: a reference-standard dataset built using the CORD-19 and LitCOVID initiatives, and a gold-standard dataset prepared by the experts from a public health domain. The CoQUAD has a Retriever component trained on the BM25 algorithm that searches the reference-standard dataset for relevant documents based on a question related to COVID-19. CoQUAD also has a Reader component that consists of a Transformer-based model, namely MPNet, which is used to read the paragraphs and find the answers related to a question from the retrieved documents. In comparison to previous works, the proposed CoQUAD system can answer questions related to early, mid, and post-COVID-19 topics.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">Extensive experiments on CoQUAD Retriever and Reader modules show that CoQUAD can provide effective and relevant answers to any COVID-19-related questions posed in natural language, with a higher level of accuracy. When compared to state-of-the-art baselines, CoQUAD outperforms the previous models, achieving an exact match ratio score of 77.50% and an F1 score of 77.10%.</AbstractText><AbstractText Label="CONCLUSION" NlmCategory="CONCLUSIONS">CoQUAD is a question-answering system that mines COVID-19 literature using natural language processing techniques to help the research community find the most recent findings and answer any related questions.</AbstractText><CopyrightInformation>&#xa9; 2022. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Raza</LastName><ForeName>Shaina</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Public Health Ontario (PHO), Toronto, ON, Canada. shaina.raza@oahpp.ca.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Dalla Lana School of Public Health, University of Toronto, Toronto, ON, Canada. shaina.raza@oahpp.ca.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Schwartz</LastName><ForeName>Brian</ForeName><Initials>B</Initials><AffiliationInfo><Affiliation>Public Health Ontario (PHO), Toronto, ON, Canada.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Dalla Lana School of Public Health, University of Toronto, Toronto, ON, Canada.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Rosella</LastName><ForeName>Laura C</ForeName><Initials>LC</Initials><AffiliationInfo><Affiliation>Dalla Lana School of Public Health, University of Toronto, Toronto, ON, Canada.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>06</Month><Day>02</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>BMC Bioinformatics</MedlineTA><NlmUniqueID>100965194</NlmUniqueID><ISSNLinking>1471-2105</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D000465" MajorTopicYN="N">Algorithms</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D019985" MajorTopicYN="Y">Benchmarking</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000086382" MajorTopicYN="Y">COVID-19</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D007802" MajorTopicYN="N">Language</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D009323" MajorTopicYN="N">Natural Language Processing</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">CORD-19</Keyword><Keyword MajorTopicYN="N">COVID-19</Keyword><Keyword MajorTopicYN="N">LitCOVID</Keyword><Keyword MajorTopicYN="N">Long-COVID</Keyword><Keyword MajorTopicYN="N">Pipeline</Keyword><Keyword MajorTopicYN="N">Post-COVID-19</Keyword><Keyword MajorTopicYN="N">Question answering system</Keyword><Keyword MajorTopicYN="N">Transformer model</Keyword></KeywordList><CoiStatement>The authors declare that they have no competing interests.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>3</Month><Day>10</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>5</Month><Day>26</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2022</Year><Month>6</Month><Day>2</Day><Hour>23</Hour><Minute>38</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2022</Year><Month>6</Month><Day>3</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2022</Year><Month>6</Month><Day>7</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="pmc-release"><Year>2022</Year><Month>6</Month><Day>2</Day></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">35655148</ArticleId><ArticleId IdType="pmc">PMC9160513</ArticleId><ArticleId IdType="doi">10.1186/s12859-022-04751-6</ArticleId><ArticleId IdType="pii">10.1186/s12859-022-04751-6</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Yuki K, Fujiogi M, Koutsogiannaki S. COVID-19 pathophysiology: a review. Clin Immunol. 2020;215:108427. doi: 10.1016/j.clim.2020.108427.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.clim.2020.108427</ArticleId><ArticleId IdType="pmc">PMC7169933</ArticleId><ArticleId IdType="pubmed">32325252</ArticleId></ArticleIdList></Reference><Reference><Citation>Ksiazek TG, Erdman D, Goldsmith CS, Zaki SR, Peret T, Emery S, et al. A novel coronavirus associated with severe acute respiratory syndrome. N Engl J Med. 2003;348(20):1953&#x2013;1966. doi: 10.1056/NEJMoa030781.</Citation><ArticleIdList><ArticleId IdType="doi">10.1056/NEJMoa030781</ArticleId><ArticleId IdType="pubmed">12690092</ArticleId></ArticleIdList></Reference><Reference><Citation>World Health Organization. Archived: WHO Timeline&#x2014;COVID-19 [Internet]. Wold Health Organization. 2020 [cited 2021 Oct 7]. p. 2020. Available from: https://www.who.int/news/item/27-04-2020-who-timeline---covid-19</Citation></Reference><Reference><Citation>Rajkumar RP. COVID-19 and mental health: a review of the existing literature. Asian J Psychiatr. 2020;52:102066. doi: 10.1016/j.ajp.2020.102066.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ajp.2020.102066</ArticleId><ArticleId IdType="pmc">PMC7151415</ArticleId><ArticleId IdType="pubmed">32302935</ArticleId></ArticleIdList></Reference><Reference><Citation>Lopez-Leon S, Wegman-Ostrosky T, Perelman C, Sepulveda R, Rebolledo PA, Cuapio A, et al. More than 50 long-term effects of COVID-19: a systematic review and meta-analysis. Res Sq. 2021;32:1613. doi: 10.1101/2021.01.27.21250617.</Citation><ArticleIdList><ArticleId IdType="doi">10.1101/2021.01.27.21250617</ArticleId><ArticleId IdType="pmc">PMC8352980</ArticleId><ArticleId IdType="pubmed">34373540</ArticleId></ArticleIdList></Reference><Reference><Citation>Akbarialiabad H, Taghrir MH, Abdollahi A, Ghahramani N, Kumar M, Paydar S, et al. Long COVID, a comprehensive systematic scoping review. Infection. 2021;49:1163&#x2013;1186. doi: 10.1007/s15010-021-01666-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s15010-021-01666-x</ArticleId><ArticleId IdType="pmc">PMC8317481</ArticleId><ArticleId IdType="pubmed">34319569</ArticleId></ArticleIdList></Reference><Reference><Citation>Nalbandian A, Sehgal K, Gupta A, Madhavan MV, McGroder C, Stevens JS, et al. Post-acute COVID-19 syndrome. Nat Med. 2021;27(4):601&#x2013;15. doi: 10.1038/s41591-021-01283-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41591-021-01283-z</ArticleId><ArticleId IdType="pmc">PMC8893149</ArticleId><ArticleId IdType="pubmed">33753937</ArticleId></ArticleIdList></Reference><Reference><Citation>World Health Organization, EPI-Win, Infodemic. Clinical long-term effects of COVID-19. 2021;(March 26):15. Available from: www.who.int/epi-win</Citation></Reference><Reference><Citation>CDC. Post-COVID Conditions: Information for Healthcare Providers. US Dep Heal Hum Serv [Internet]. 2021 [cited 2021 Dec 7]; 2019&#x2013;21. Available from: https://www.cdc.gov/coronavirus/2019-ncov/hcp/clinical-care/post-covid-conditions.html</Citation></Reference><Reference><Citation>Else H. How a torrent of COVID science changed research publishing&#x2014;in seven charts. Nature. 2020;588(7839):553. doi: 10.1038/d41586-020-03564-y.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/d41586-020-03564-y</ArticleId><ArticleId IdType="pubmed">33328621</ArticleId></ArticleIdList></Reference><Reference><Citation>Gianola S, Jesus TS, Bargeri S, Castellini G. Characteristics of academic publications, preprints, and registered clinical trials on the COVID-19 pandemic. PLOS ONE. 2020;15:0240123. doi: 10.1371/journal.pone.0240123.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0240123</ArticleId><ArticleId IdType="pmc">PMC7537872</ArticleId><ArticleId IdType="pubmed">33022014</ArticleId></ArticleIdList></Reference><Reference><Citation>De Maio C, Fenza G, Gallo M, Loia V, Volpe A. Cross-relating heterogeneous Text Streams for Credibility Assessment. IEEE conference on evolving and adaptive intelligent systems 2020; 2020-May.</Citation></Reference><Reference><Citation>Radvan M, Barte&#x10d;k&#x16f; E, S&#xfd;korov&#xe1; U, Pa&#x159;&#xed;zkov&#xe1; R, Richter S, Kamen&#xed;k M, et al. Follow-up care after COVID-19 and its related concerns. Vnitr Lek. 2021;67(1):30&#x2013;6. doi: 10.36290/vnl.2021.004.</Citation><ArticleIdList><ArticleId IdType="doi">10.36290/vnl.2021.004</ArticleId><ArticleId IdType="pubmed">33752388</ArticleId></ArticleIdList></Reference><Reference><Citation>Science Table. Ontario dashboard&#x2014;Ontario COVID-19 science advisory table [Internet]. 2021. Available from: https://covid19-sciencetable.ca/ontario-dashboard/#riskbyvaccinationstatus</Citation></Reference><Reference><Citation>Bouziane A, Bouchiha D, Doumi N, Malki M. Question answering systems: survey and trends. Procedia Comput Sci. 2015;73:366&#x2013;75. doi: 10.1016/j.procs.2015.12.005.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.procs.2015.12.005</ArticleId></ArticleIdList></Reference><Reference><Citation>Peters MDJ, Marnie C, Tricco AC, Pollock D, Munn Z, Alexander L, et al. Updated methodological guidance for the conduct of scoping reviews. JBI Evid Synth. 2020;18(10):2119&#x2013;26. doi: 10.11124/JBIES-20-00167.</Citation><ArticleIdList><ArticleId IdType="doi">10.11124/JBIES-20-00167</ArticleId><ArticleId IdType="pubmed">33038124</ArticleId></ArticleIdList></Reference><Reference><Citation>Lewis P, Denoyer L, Riedel S. Unsupervised question answering by cloze translation. In: Annual meeting of the association for computational linguistics proceedings conference; 2020. p. 4896&#x2013;910.</Citation></Reference><Reference><Citation>Lewis P, O&#x11f;uz B, Rinott R, Riedel S, Schwenk H. MLQA: Evaluating cross-lingual extractive question answering. http://arxiv.org/abs/1910.07475. 2019.</Citation></Reference><Reference><Citation>Brady EL, Wallenstein MB. The national standard reference data system. Science. 1967;156(3776):754&#x2013;62. doi: 10.1126/science.156.3776.754.</Citation><ArticleIdList><ArticleId IdType="doi">10.1126/science.156.3776.754</ArticleId><ArticleId IdType="pubmed">6022226</ArticleId></ArticleIdList></Reference><Reference><Citation>Cohen T, Roberts K, Gururaj AE, Chen X, Pournejati S, Alter G, et al. A publicly available benchmark for biomedical dataset retrieval: the reference standard for the 2016 bioCADDIE dataset retrieval challenge. Database (Oxford) 2017;2017:1&#x2013;10. doi: 10.1093/database/bax061.</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/database/bax061</ArticleId><ArticleId IdType="pmc">PMC5737202</ArticleId><ArticleId IdType="pubmed">29220453</ArticleId></ArticleIdList></Reference><Reference><Citation>Cardoso JR, Pereira LM, Iversen MD, Ramos AL. What is gold standard and what is ground truth? Dental Press J Orthod. 2014;19:27&#x2013;30. doi: 10.1590/2176-9451.19.5.027-030.ebo.</Citation><ArticleIdList><ArticleId IdType="doi">10.1590/2176-9451.19.5.027-030.ebo</ArticleId><ArticleId IdType="pmc">PMC4296658</ArticleId><ArticleId IdType="pubmed">25715714</ArticleId></ArticleIdList></Reference><Reference><Citation>Alzubi JA, Jain R, Singh A, Parwekar P, Gupta M. COBERT: COVID-19 question answering system using BERT. Arab J Sci Eng. 2021;19.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8220121</ArticleId><ArticleId IdType="pubmed">34178569</ArticleId></ArticleIdList></Reference><Reference><Citation>Ngai H, Park Y, Chen J, Parsapoor M. Transformer-based models for question answering on COVID19. 2021;1&#x2013;7. Available from: http://arxiv.org/abs/2101.11432</Citation></Reference><Reference><Citation>Saikh T, Sahoo SK, Ekbal A, Bhattacharyya P. COVIDRead: a large-scale question answering dataset on COVID-19. 2021; Available from: http://arxiv.org/abs/2110.09321</Citation></Reference><Reference><Citation>Tang R, Nogueira R, Zhang E, Gupta N, Cam P, Cho K, et al. Rapidly bootstrapping a question answering dataset for COVID-19. 2020; Available from: http://arxiv.org/abs/2004.11339</Citation></Reference><Reference><Citation>Lu Wang L, Lo K, Chandrasekhar Y, Reas R, Yang J, Eide D, et al. CORD-19: The Covid-19 Open Research Dataset. [Internet]. 2020. Available from: http://www.ncbi.nlm.nih.gov/pubmed/32510522; http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC7251955</Citation></Reference><Reference><Citation>Chen Q, Allot A, Lu Z. LitCovid: An open database of COVID-19 literature. Nucleic Acids Res. 2021;49(D1):D1534&#x2013;D1540. doi: 10.1093/nar/gkaa952.</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/nar/gkaa952</ArticleId><ArticleId IdType="pmc">PMC7778958</ArticleId><ArticleId IdType="pubmed">33166392</ArticleId></ArticleIdList></Reference><Reference><Citation>Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. In: Advances in neural information processing systems; 2017. p. 5998&#x2013;6008.</Citation></Reference><Reference><Citation>M&#xf6;ller T, Anthony Reina G, Jayakumar Lawrence Livermore R, Liu Y, Ott M, Goyal N, et al. COVID-QA: a question answering dataset for COVID-19. 2020;2383&#x2013;92. Available from: https://github.com/deepset-ai/COVID-QA.</Citation></Reference><Reference><Citation>Song K, Tan X, Qin T, Lu J, Liu T-Y. Mpnet: Masked and permuted pre-training for language understanding. http://arxiv.org/abs/2004.09297. 2020.</Citation></Reference><Reference><Citation>Rajpurkar P, Zhang J, Lopyrev K, Liang P. SQuad: 100,000+ questions for machine comprehension of text. In: EMNLP 2016 - conference on empirical methods in natural language processing, proceedings. 2016. p. 2383&#x2013;92.</Citation></Reference><Reference><Citation>Diefenbach D, Lopez V, Singh K, Maret P, Diefenbach D, Lopez V, et al. Core techniques of question answering systems over knowledge bases: a survey to cite this version&#x202f;: HAL Id&#x202f;: hal-01637143 core techniques of question answering systems over knowledge bases: a survey. 2017;</Citation></Reference><Reference><Citation>Badugu S, Manivannan R. A study on different closed domain question answering approaches. Int J Speech Technol. 2020;23:315&#x2013;325. doi: 10.1007/s10772-020-09692-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10772-020-09692-0</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen D, Yih W. Open-domain question answering. In: Proceedings of the 58th annual meeting of the association for computational linguistics: tutorial abstracts; 2020. p. 34&#x2013;7.</Citation></Reference><Reference><Citation>Teufel S. An overview of evaluation methods in TREC ad hoc information retrieval and TREC question answering. In: Evaluation of text and speech systems; 2007. p. 163&#x2013;86.</Citation></Reference><Reference><Citation>Lee K, Salant S, Kwiatkowski T, Parikh A, Das D, Berant J. Learning recurrent span representations for extractive question answering. 2016;1&#x2013;9. Available from: http://arxiv.org/abs/1611.01436</Citation></Reference><Reference><Citation>Colavizza G. Covid-19 research in wikipedia. Quant Sci Stud. 2020;1(4):1349&#x2013;1380. doi: 10.1162/qss_a_00080.</Citation><ArticleIdList><ArticleId IdType="doi">10.1162/qss_a_00080</ArticleId></ArticleIdList></Reference><Reference><Citation>Song G, Wang Y. A hybrid model for medical paper summarization based on COVID-19 open research dataset. In: 2020 4th International conference on computer science and artificial intelligence; 2020. p. 52&#x2013;6.</Citation></Reference><Reference><Citation>Esteva A, Kale A, Paulus R, Hashimoto K, Yin W, Radev D, et al. COVID-19 information retrieval with deep-learning based semantic search, question answering, and abstractive summarization. npj Digit Med. 2021;4(1):1&#x2013;10. doi: 10.1038/s41746-020-00373-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41746-020-00373-5</ArticleId><ArticleId IdType="pmc">PMC8041998</ArticleId><ArticleId IdType="pubmed">33846532</ArticleId></ArticleIdList></Reference><Reference><Citation>Zdravkovic SA, Duong CT, Hellenbrand AA, Duff SR, Dreger AL. Establishment of a reference standard database for use in the qualitative and semi-quantitative analysis of pharmaceutical contact materials within an extractables survey by GC&#x2013;MS. J Pharm Biomed Anal. 2018;151:49&#x2013;60. doi: 10.1016/j.jpba.2017.12.054.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jpba.2017.12.054</ArticleId><ArticleId IdType="pubmed">29306734</ArticleId></ArticleIdList></Reference><Reference><Citation>Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of deep bidirectional transformers for language understanding. http://arxiv.org/abs/1810.04805. 2018.</Citation></Reference><Reference><Citation>Wu X, Lode M. Language models are unsupervised multitask learners (summarization). OpenAI Blog [Internet]. 2020 [cited 2020 Dec 26];1(May):1&#x2013;7. Available from: https://github.com/codelucas/newspaper</Citation></Reference><Reference><Citation>Torrey L, Shavlik J. Transfer learning. In: Handbook of research on machine learning applications and trends: algorithms, methods, and techniques. IGI Global; 2010. p. 242&#x2013;64.</Citation></Reference><Reference><Citation>Oniani D, Wang Y. A Qualitative evaluation of language models on automatic question-answering for COVID-19. In: Proceedings of the 11th ACM International conference on bioinformatics, computer biology heal informatics, BCB 2020. 2020</Citation></Reference><Reference><Citation>Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, et al. Roberta: A robustly optimized bert pretraining approach. http://arxiv.org/abs/1907.11692. 2019.</Citation></Reference><Reference><Citation>Voorhees E, Alam T, Bedrick S, Demner-Fushman D, Hersh WR, Lo K, et al. TREC-COVID: Constructing a pandemic information retrieval test collection. 2020;1&#x2013;10. Available from: http://arxiv.org/abs/2005.04474</Citation></Reference><Reference><Citation>World Health Organization. Global research on coronavirus disease (COVID-19) [Internet]. 2021 [cited 2021 Dec 30]. Available from: https://www.who.int/emergencies/diseases/novel-coronavirus-2019/global-research-on-novel-coronavirus-2019-ncov</Citation></Reference><Reference><Citation>Cunningham E, Smyth B, Greene D. Collaboration in the time of COVID: a scientometric analysis of multidisciplinary SARS-CoV-2 research. Humanit Soc Sci Commun. 2021;8(1):1&#x2013;8. doi: 10.1057/s41599-020-00684-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1057/s41599-020-00684-8</ArticleId><ArticleId IdType="pmc">PMC8573561</ArticleId><ArticleId IdType="pubmed">34780583</ArticleId></ArticleIdList></Reference><Reference><Citation>Campillos-Llanos L, Valverde-Mateos A, Capllonch-Carri&#xf3;n A, Moreno-Sandoval A. A clinical trials corpus annotated with UMLS entities to enhance the access to evidence-based medicine. BMC Med Inform Decis Mak. 2021;21(1):1&#x2013;19. doi: 10.1186/s12911-021-01395-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s12911-021-01395-z</ArticleId><ArticleId IdType="pmc">PMC7898014</ArticleId><ArticleId IdType="pubmed">33618727</ArticleId></ArticleIdList></Reference><Reference><Citation>Hendrycks D, Mazeika M, Wilson D, Gimpel K. Using trusted data to train deep networks on labels corrupted by severe noise. Adv Neural Inf Process Syst. 2018;2018:10456&#x2013;65.</Citation></Reference><Reference><Citation>Hu H, Wen Y, Chua T-S, Li X. Toward scalable systems for big data analytics: a technology tutorial. IEEE Access. 2014;2:652&#x2013;687. doi: 10.1109/ACCESS.2014.2332453.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2014.2332453</ArticleId></ArticleIdList></Reference><Reference><Citation>Sch&#xfc;tze H, Manning CD, Raghavan P. Introduction to information retrieval. Cambridge: Cambridge University Press; 2008.</Citation></Reference><Reference><Citation>Chaybouti S, Saghe A, Shabou A. EfficientQA&#x202f;: a RoBERTa Based Phrase-Indexed Question-Answering System. 2021;(figure 1):1&#x2013;9. Available from: http://arxiv.org/abs/2101.02157</Citation></Reference><Reference><Citation>Robertson S, Zaragoza H. The probabilistic relevance framework: BM25 and beyond. Delft: Now Publishers Inc; 2009.</Citation></Reference><Reference><Citation>Robertson SE, Sp&#xe4;rck Jones K. Simple, proven approaches to text retrieval. 1994.</Citation></Reference><Reference><Citation>Aggarwal CC. Data mining: the textbook. Berlin: Springer; 2015.</Citation></Reference><Reference><Citation>Yang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov RR, Le Q V. Xlnet: Generalized autoregressive pretraining for language understanding. In: Advances in neural information processing systems. 2019. p. 5753&#x2013;63.</Citation></Reference><Reference><Citation>Lewis M, Liu Y, Goyal N, Ghazvininejad M, Mohamed A, Levy O, et al. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. http://arxiv.org/abs/1910.13461. 2019.</Citation></Reference><Reference><Citation>Rogers A, Kovaleva O, Rumshisky A. A primer in bertology: what we know about how bert works. Trans Assoc Comput Linguist. 2020;8:842&#x2013;866. doi: 10.1162/tacl_a_00349.</Citation><ArticleIdList><ArticleId IdType="doi">10.1162/tacl_a_00349</ArticleId></ArticleIdList></Reference><Reference><Citation>Dai Z, Yang Z, Yang Y, Carbonell J, Le Q V, Salakhutdinov R. Transformer-xl: Attentive language models beyond a fixed-length context. http://arxiv.org/abs/1901.02860. 2019.</Citation></Reference><Reference><Citation>Lan Z, Chen M, Goodman S, Gimpel K, Sharma P, Soricut R. Albert: A lite bert for self-supervised learning of language representations. http://arxiv.org/abs/1909.11942. 2019.</Citation></Reference><Reference><Citation>Clark K, Luong M-T, Le Q V, Manning CD. Electra: Pre-training text encoders as discriminators rather than generators. http://arxiv.org/abs/2003.10555. 2020;</Citation></Reference><Reference><Citation>Dai Z, Lai G, Yang Y, Le QV. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. http://arxiv.org/abs/2006.03236. 2020.</Citation></Reference><Reference><Citation>Beltagy I, Peters ME, Cohan A. Longformer: The long-document transformer. http://arxiv.org/abs/2004.05150. 2020.</Citation></Reference><Reference><Citation>Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, et al. RoBERTa: A robustly optimized BERT pre-training approach. 2019 [cited 2021 Dec 20];2383&#x2013;92. Available from: https://github.com/deepset-ai/COVID-QA.</Citation></Reference><Reference><Citation>Risch J, M&#xf6;ller T, Gutsch J, Pietsch M. Semantic answer similarity for evaluating question answering models. 2021; Available from: http://arxiv.org/abs/2108.06130</Citation></Reference><Reference><Citation>Kingma DP, Ba JL. Adam: A method for stochastic optimization. In: 3rd International conference on learning representations, ICLR 2015&#x2014;Conference track proceedings; 2015.</Citation></Reference><Reference><Citation>Baeza-Yates R, Ribeiro-Neto B, et al. Modern information retrieval. New York: ACM Press; 1999.</Citation></Reference><Reference><Citation>El-Geish M. Gestalt: a Stacking Ensemble for SQuAD2.0. 2020;1&#x2013;11. Available from: http://arxiv.org/abs/2004.07067</Citation></Reference><Reference><Citation>Kejriwal M. What is a knowledge graph? SpringerBriefs in Computer Science. 2019. p. 1&#x2013;7.</Citation></Reference><Reference><Citation>Burls A. What is critical appraisal? Citeseer; 2014.</Citation></Reference><Reference><Citation>Wynants L, Van Calster B, Collins GS, Riley RD, Heinze G, Schuit E, et al. Prediction models for diagnosis and prognosis of covid-19: systematic review and critical appraisal. BMJ. 2020;369(July).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7222643</ArticleId><ArticleId IdType="pubmed">32265220</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle></PubmedArticleSet>